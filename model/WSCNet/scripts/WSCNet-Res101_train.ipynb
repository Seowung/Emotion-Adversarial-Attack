{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.0.0\n",
      "Torchvision Version:  0.2.2\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms \n",
    "#   to the ImageFolder structure\n",
    "data_dir = '../dataset/FI/'\n",
    "#train:19292 val:3408 total:22700\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 8\n",
    "\n",
    "# Number of feature map in each class\n",
    "num_maps = 4\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 10\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 25\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = False\n",
    "\n",
    "model_savepath = '../models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_corrects_dec = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs1, outputs2 = model(inputs)\n",
    "                        loss1 = criterion(outputs1, labels)\n",
    "                        loss2 = criterion(outputs2, labels)\n",
    "                        loss = 0.5*loss1 + 0.5*loss2\n",
    "\n",
    "                    _, preds = torch.max(outputs2, 1)\n",
    "                    _, preds_dec = torch.max(outputs1, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                running_corrects_dec += torch.sum(preds_dec == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler_ft.step() #scheduler_ft.step(loss)\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_acc_dec = running_corrects_dec.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} Dec: {:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_acc_dec))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            if phase == 'val':\n",
    "                #writer.add_scalar('epoch_valloss', epoch_loss, global_step = epoch)\n",
    "                #writer.add_scalar('epoch_valacc', epoch_acc, global_step = epoch)\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            #else:\n",
    "                #writer.add_scalar('epoch_trainloss', epoch_loss, global_step = epoch)\n",
    "                #writer.add_scalar('epoch_traincc', epoch_acc, global_step = epoch)\n",
    "                #writer.add_scalar('lr_train', lr, global_step = epoch)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetWSL(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, num_classes, num_maps, pooling, pooling2):\n",
    "        super(ResNetWSL, self).__init__()\n",
    "        self.features = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.num_ftrs = model.fc.in_features\n",
    "\n",
    "        self.downconv = nn.Sequential(\n",
    "            nn.Conv2d(2048, num_classes*num_maps, kernel_size=1, stride=1, padding=0, bias=True))\n",
    "        \n",
    "        self.GAP = nn.AvgPool2d(14)\n",
    "        self.GMP = nn.MaxPool2d(14)\n",
    "        self.spatial_pooling = pooling\n",
    "        self.spatial_pooling2 = pooling2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4096, num_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x_ori = x  \n",
    "        # detect branch\n",
    "        x = self.downconv(x) \n",
    "        x_conv = x              \n",
    "        x = self.GAP(x)  #x = self.GMP(x)       \n",
    "        x = self.spatial_pooling(x) \n",
    "        x = x.view(x.size(0), -1)\n",
    "        # cls branch\n",
    "        x_conv = self.spatial_pooling(x_conv) \n",
    "        x_conv = x_conv * x.view(x.size(0),x.size(1),1,1) \n",
    "        x_conv = self.spatial_pooling2(x_conv) \n",
    "        x_conv_copy = x_conv\n",
    "        for num in range(0,2047):            \n",
    "            x_conv_copy = torch.cat((x_conv_copy, x_conv),1) \n",
    "        x_conv_copy = torch.mul(x_conv_copy,x_ori)\n",
    "        x_conv_copy = torch.cat((x_ori,x_conv_copy),1) \n",
    "        x_conv_copy = self.GAP(x_conv_copy)\n",
    "        x_conv_copy = x_conv_copy.view(x_conv_copy.size(0),-1)\n",
    "        x_conv_copy = self.classifier(x_conv_copy)\n",
    "        return x, x_conv_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function, Variable\n",
    "class ClassWisePoolFunction(Function):\n",
    "    def __init__(self, num_maps):\n",
    "        super(ClassWisePoolFunction, self).__init__()\n",
    "        self.num_maps = num_maps\n",
    "\n",
    "    def forward(self, input):\n",
    "        # batch dimension\n",
    "        batch_size, num_channels, h, w = input.size()\n",
    "\n",
    "        if num_channels % self.num_maps != 0:\n",
    "            print('Error in ClassWisePoolFunction. The number of channels has to be a multiple of the number of maps per class')\n",
    "            sys.exit(-1)\n",
    "\n",
    "        num_outputs = int(num_channels / self.num_maps)\n",
    "        x = input.view(batch_size, num_outputs, self.num_maps, h, w)\n",
    "        output = torch.sum(x, 2)\n",
    "        self.save_for_backward(input)\n",
    "        return output.view(batch_size, num_outputs, h, w) / self.num_maps\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        input, = self.saved_tensors\n",
    "        # batch dimension\n",
    "        batch_size, num_channels, h, w = input.size()\n",
    "        num_outputs = grad_output.size(1)\n",
    "\n",
    "        grad_input = grad_output.view(batch_size, num_outputs, 1, h, w).expand(batch_size, num_outputs, self.num_maps,\n",
    "                                                                               h, w).contiguous()\n",
    "        return grad_input.view(batch_size, num_channels, h, w)\n",
    "\n",
    "class ClassWisePool(nn.Module):\n",
    "    def __init__(self, num_maps):\n",
    "        super(ClassWisePool, self).__init__()\n",
    "        self.num_maps = num_maps\n",
    "\n",
    "    def forward(self, input):\n",
    "        return ClassWisePoolFunction(self.num_maps)(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNetWSL(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (6): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (7): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (8): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (9): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (10): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (11): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (12): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (13): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (14): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (15): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (16): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (17): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (18): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (19): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (20): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (21): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (22): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (downconv): Sequential(\n",
      "    (0): Conv2d(2048, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (GAP): AvgPool2d(kernel_size=14, stride=14, padding=0)\n",
      "  (GMP): MaxPool2d(kernel_size=14, stride=14, padding=0, dilation=1, ceil_mode=False)\n",
      "  (spatial_pooling): Sequential(\n",
      "    (class_wise): ClassWisePool (num_maps=4)\n",
      "  )\n",
      "  (spatial_pooling2): Sequential(\n",
      "    (class_wise): ClassWisePool (num_maps=8)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "def initialize_model(model_name, num_classes, num_maps, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "    \n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet101\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet101(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "\n",
    "        pooling = nn.Sequential()\n",
    "        pooling.add_module('class_wise', ClassWisePool(num_maps))\n",
    "        pooling2 = nn.Sequential()\n",
    "        pooling2.add_module('class_wise', ClassWisePool(num_classes))\n",
    "        model_ft = ResNetWSL(model_ft, num_classes, num_maps, pooling, pooling2)\n",
    "        \n",
    "        input_size = 448\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, num_maps, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n",
      "['amuse', 'anger', 'awe', 'content', 'disgust', 'excit', 'fear', 'sad']\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=8) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "\n",
    "#image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU  cuda:0\n",
      "Params to learn:\n",
      "\t features.0.weight\n",
      "\t features.1.weight\n",
      "\t features.1.bias\n",
      "\t features.4.0.conv1.weight\n",
      "\t features.4.0.bn1.weight\n",
      "\t features.4.0.bn1.bias\n",
      "\t features.4.0.conv2.weight\n",
      "\t features.4.0.bn2.weight\n",
      "\t features.4.0.bn2.bias\n",
      "\t features.4.0.conv3.weight\n",
      "\t features.4.0.bn3.weight\n",
      "\t features.4.0.bn3.bias\n",
      "\t features.4.0.downsample.0.weight\n",
      "\t features.4.0.downsample.1.weight\n",
      "\t features.4.0.downsample.1.bias\n",
      "\t features.4.1.conv1.weight\n",
      "\t features.4.1.bn1.weight\n",
      "\t features.4.1.bn1.bias\n",
      "\t features.4.1.conv2.weight\n",
      "\t features.4.1.bn2.weight\n",
      "\t features.4.1.bn2.bias\n",
      "\t features.4.1.conv3.weight\n",
      "\t features.4.1.bn3.weight\n",
      "\t features.4.1.bn3.bias\n",
      "\t features.4.2.conv1.weight\n",
      "\t features.4.2.bn1.weight\n",
      "\t features.4.2.bn1.bias\n",
      "\t features.4.2.conv2.weight\n",
      "\t features.4.2.bn2.weight\n",
      "\t features.4.2.bn2.bias\n",
      "\t features.4.2.conv3.weight\n",
      "\t features.4.2.bn3.weight\n",
      "\t features.4.2.bn3.bias\n",
      "\t features.5.0.conv1.weight\n",
      "\t features.5.0.bn1.weight\n",
      "\t features.5.0.bn1.bias\n",
      "\t features.5.0.conv2.weight\n",
      "\t features.5.0.bn2.weight\n",
      "\t features.5.0.bn2.bias\n",
      "\t features.5.0.conv3.weight\n",
      "\t features.5.0.bn3.weight\n",
      "\t features.5.0.bn3.bias\n",
      "\t features.5.0.downsample.0.weight\n",
      "\t features.5.0.downsample.1.weight\n",
      "\t features.5.0.downsample.1.bias\n",
      "\t features.5.1.conv1.weight\n",
      "\t features.5.1.bn1.weight\n",
      "\t features.5.1.bn1.bias\n",
      "\t features.5.1.conv2.weight\n",
      "\t features.5.1.bn2.weight\n",
      "\t features.5.1.bn2.bias\n",
      "\t features.5.1.conv3.weight\n",
      "\t features.5.1.bn3.weight\n",
      "\t features.5.1.bn3.bias\n",
      "\t features.5.2.conv1.weight\n",
      "\t features.5.2.bn1.weight\n",
      "\t features.5.2.bn1.bias\n",
      "\t features.5.2.conv2.weight\n",
      "\t features.5.2.bn2.weight\n",
      "\t features.5.2.bn2.bias\n",
      "\t features.5.2.conv3.weight\n",
      "\t features.5.2.bn3.weight\n",
      "\t features.5.2.bn3.bias\n",
      "\t features.5.3.conv1.weight\n",
      "\t features.5.3.bn1.weight\n",
      "\t features.5.3.bn1.bias\n",
      "\t features.5.3.conv2.weight\n",
      "\t features.5.3.bn2.weight\n",
      "\t features.5.3.bn2.bias\n",
      "\t features.5.3.conv3.weight\n",
      "\t features.5.3.bn3.weight\n",
      "\t features.5.3.bn3.bias\n",
      "\t features.6.0.conv1.weight\n",
      "\t features.6.0.bn1.weight\n",
      "\t features.6.0.bn1.bias\n",
      "\t features.6.0.conv2.weight\n",
      "\t features.6.0.bn2.weight\n",
      "\t features.6.0.bn2.bias\n",
      "\t features.6.0.conv3.weight\n",
      "\t features.6.0.bn3.weight\n",
      "\t features.6.0.bn3.bias\n",
      "\t features.6.0.downsample.0.weight\n",
      "\t features.6.0.downsample.1.weight\n",
      "\t features.6.0.downsample.1.bias\n",
      "\t features.6.1.conv1.weight\n",
      "\t features.6.1.bn1.weight\n",
      "\t features.6.1.bn1.bias\n",
      "\t features.6.1.conv2.weight\n",
      "\t features.6.1.bn2.weight\n",
      "\t features.6.1.bn2.bias\n",
      "\t features.6.1.conv3.weight\n",
      "\t features.6.1.bn3.weight\n",
      "\t features.6.1.bn3.bias\n",
      "\t features.6.2.conv1.weight\n",
      "\t features.6.2.bn1.weight\n",
      "\t features.6.2.bn1.bias\n",
      "\t features.6.2.conv2.weight\n",
      "\t features.6.2.bn2.weight\n",
      "\t features.6.2.bn2.bias\n",
      "\t features.6.2.conv3.weight\n",
      "\t features.6.2.bn3.weight\n",
      "\t features.6.2.bn3.bias\n",
      "\t features.6.3.conv1.weight\n",
      "\t features.6.3.bn1.weight\n",
      "\t features.6.3.bn1.bias\n",
      "\t features.6.3.conv2.weight\n",
      "\t features.6.3.bn2.weight\n",
      "\t features.6.3.bn2.bias\n",
      "\t features.6.3.conv3.weight\n",
      "\t features.6.3.bn3.weight\n",
      "\t features.6.3.bn3.bias\n",
      "\t features.6.4.conv1.weight\n",
      "\t features.6.4.bn1.weight\n",
      "\t features.6.4.bn1.bias\n",
      "\t features.6.4.conv2.weight\n",
      "\t features.6.4.bn2.weight\n",
      "\t features.6.4.bn2.bias\n",
      "\t features.6.4.conv3.weight\n",
      "\t features.6.4.bn3.weight\n",
      "\t features.6.4.bn3.bias\n",
      "\t features.6.5.conv1.weight\n",
      "\t features.6.5.bn1.weight\n",
      "\t features.6.5.bn1.bias\n",
      "\t features.6.5.conv2.weight\n",
      "\t features.6.5.bn2.weight\n",
      "\t features.6.5.bn2.bias\n",
      "\t features.6.5.conv3.weight\n",
      "\t features.6.5.bn3.weight\n",
      "\t features.6.5.bn3.bias\n",
      "\t features.6.6.conv1.weight\n",
      "\t features.6.6.bn1.weight\n",
      "\t features.6.6.bn1.bias\n",
      "\t features.6.6.conv2.weight\n",
      "\t features.6.6.bn2.weight\n",
      "\t features.6.6.bn2.bias\n",
      "\t features.6.6.conv3.weight\n",
      "\t features.6.6.bn3.weight\n",
      "\t features.6.6.bn3.bias\n",
      "\t features.6.7.conv1.weight\n",
      "\t features.6.7.bn1.weight\n",
      "\t features.6.7.bn1.bias\n",
      "\t features.6.7.conv2.weight\n",
      "\t features.6.7.bn2.weight\n",
      "\t features.6.7.bn2.bias\n",
      "\t features.6.7.conv3.weight\n",
      "\t features.6.7.bn3.weight\n",
      "\t features.6.7.bn3.bias\n",
      "\t features.6.8.conv1.weight\n",
      "\t features.6.8.bn1.weight\n",
      "\t features.6.8.bn1.bias\n",
      "\t features.6.8.conv2.weight\n",
      "\t features.6.8.bn2.weight\n",
      "\t features.6.8.bn2.bias\n",
      "\t features.6.8.conv3.weight\n",
      "\t features.6.8.bn3.weight\n",
      "\t features.6.8.bn3.bias\n",
      "\t features.6.9.conv1.weight\n",
      "\t features.6.9.bn1.weight\n",
      "\t features.6.9.bn1.bias\n",
      "\t features.6.9.conv2.weight\n",
      "\t features.6.9.bn2.weight\n",
      "\t features.6.9.bn2.bias\n",
      "\t features.6.9.conv3.weight\n",
      "\t features.6.9.bn3.weight\n",
      "\t features.6.9.bn3.bias\n",
      "\t features.6.10.conv1.weight\n",
      "\t features.6.10.bn1.weight\n",
      "\t features.6.10.bn1.bias\n",
      "\t features.6.10.conv2.weight\n",
      "\t features.6.10.bn2.weight\n",
      "\t features.6.10.bn2.bias\n",
      "\t features.6.10.conv3.weight\n",
      "\t features.6.10.bn3.weight\n",
      "\t features.6.10.bn3.bias\n",
      "\t features.6.11.conv1.weight\n",
      "\t features.6.11.bn1.weight\n",
      "\t features.6.11.bn1.bias\n",
      "\t features.6.11.conv2.weight\n",
      "\t features.6.11.bn2.weight\n",
      "\t features.6.11.bn2.bias\n",
      "\t features.6.11.conv3.weight\n",
      "\t features.6.11.bn3.weight\n",
      "\t features.6.11.bn3.bias\n",
      "\t features.6.12.conv1.weight\n",
      "\t features.6.12.bn1.weight\n",
      "\t features.6.12.bn1.bias\n",
      "\t features.6.12.conv2.weight\n",
      "\t features.6.12.bn2.weight\n",
      "\t features.6.12.bn2.bias\n",
      "\t features.6.12.conv3.weight\n",
      "\t features.6.12.bn3.weight\n",
      "\t features.6.12.bn3.bias\n",
      "\t features.6.13.conv1.weight\n",
      "\t features.6.13.bn1.weight\n",
      "\t features.6.13.bn1.bias\n",
      "\t features.6.13.conv2.weight\n",
      "\t features.6.13.bn2.weight\n",
      "\t features.6.13.bn2.bias\n",
      "\t features.6.13.conv3.weight\n",
      "\t features.6.13.bn3.weight\n",
      "\t features.6.13.bn3.bias\n",
      "\t features.6.14.conv1.weight\n",
      "\t features.6.14.bn1.weight\n",
      "\t features.6.14.bn1.bias\n",
      "\t features.6.14.conv2.weight\n",
      "\t features.6.14.bn2.weight\n",
      "\t features.6.14.bn2.bias\n",
      "\t features.6.14.conv3.weight\n",
      "\t features.6.14.bn3.weight\n",
      "\t features.6.14.bn3.bias\n",
      "\t features.6.15.conv1.weight\n",
      "\t features.6.15.bn1.weight\n",
      "\t features.6.15.bn1.bias\n",
      "\t features.6.15.conv2.weight\n",
      "\t features.6.15.bn2.weight\n",
      "\t features.6.15.bn2.bias\n",
      "\t features.6.15.conv3.weight\n",
      "\t features.6.15.bn3.weight\n",
      "\t features.6.15.bn3.bias\n",
      "\t features.6.16.conv1.weight\n",
      "\t features.6.16.bn1.weight\n",
      "\t features.6.16.bn1.bias\n",
      "\t features.6.16.conv2.weight\n",
      "\t features.6.16.bn2.weight\n",
      "\t features.6.16.bn2.bias\n",
      "\t features.6.16.conv3.weight\n",
      "\t features.6.16.bn3.weight\n",
      "\t features.6.16.bn3.bias\n",
      "\t features.6.17.conv1.weight\n",
      "\t features.6.17.bn1.weight\n",
      "\t features.6.17.bn1.bias\n",
      "\t features.6.17.conv2.weight\n",
      "\t features.6.17.bn2.weight\n",
      "\t features.6.17.bn2.bias\n",
      "\t features.6.17.conv3.weight\n",
      "\t features.6.17.bn3.weight\n",
      "\t features.6.17.bn3.bias\n",
      "\t features.6.18.conv1.weight\n",
      "\t features.6.18.bn1.weight\n",
      "\t features.6.18.bn1.bias\n",
      "\t features.6.18.conv2.weight\n",
      "\t features.6.18.bn2.weight\n",
      "\t features.6.18.bn2.bias\n",
      "\t features.6.18.conv3.weight\n",
      "\t features.6.18.bn3.weight\n",
      "\t features.6.18.bn3.bias\n",
      "\t features.6.19.conv1.weight\n",
      "\t features.6.19.bn1.weight\n",
      "\t features.6.19.bn1.bias\n",
      "\t features.6.19.conv2.weight\n",
      "\t features.6.19.bn2.weight\n",
      "\t features.6.19.bn2.bias\n",
      "\t features.6.19.conv3.weight\n",
      "\t features.6.19.bn3.weight\n",
      "\t features.6.19.bn3.bias\n",
      "\t features.6.20.conv1.weight\n",
      "\t features.6.20.bn1.weight\n",
      "\t features.6.20.bn1.bias\n",
      "\t features.6.20.conv2.weight\n",
      "\t features.6.20.bn2.weight\n",
      "\t features.6.20.bn2.bias\n",
      "\t features.6.20.conv3.weight\n",
      "\t features.6.20.bn3.weight\n",
      "\t features.6.20.bn3.bias\n",
      "\t features.6.21.conv1.weight\n",
      "\t features.6.21.bn1.weight\n",
      "\t features.6.21.bn1.bias\n",
      "\t features.6.21.conv2.weight\n",
      "\t features.6.21.bn2.weight\n",
      "\t features.6.21.bn2.bias\n",
      "\t features.6.21.conv3.weight\n",
      "\t features.6.21.bn3.weight\n",
      "\t features.6.21.bn3.bias\n",
      "\t features.6.22.conv1.weight\n",
      "\t features.6.22.bn1.weight\n",
      "\t features.6.22.bn1.bias\n",
      "\t features.6.22.conv2.weight\n",
      "\t features.6.22.bn2.weight\n",
      "\t features.6.22.bn2.bias\n",
      "\t features.6.22.conv3.weight\n",
      "\t features.6.22.bn3.weight\n",
      "\t features.6.22.bn3.bias\n",
      "\t features.7.0.conv1.weight\n",
      "\t features.7.0.bn1.weight\n",
      "\t features.7.0.bn1.bias\n",
      "\t features.7.0.conv2.weight\n",
      "\t features.7.0.bn2.weight\n",
      "\t features.7.0.bn2.bias\n",
      "\t features.7.0.conv3.weight\n",
      "\t features.7.0.bn3.weight\n",
      "\t features.7.0.bn3.bias\n",
      "\t features.7.0.downsample.0.weight\n",
      "\t features.7.0.downsample.1.weight\n",
      "\t features.7.0.downsample.1.bias\n",
      "\t features.7.1.conv1.weight\n",
      "\t features.7.1.bn1.weight\n",
      "\t features.7.1.bn1.bias\n",
      "\t features.7.1.conv2.weight\n",
      "\t features.7.1.bn2.weight\n",
      "\t features.7.1.bn2.bias\n",
      "\t features.7.1.conv3.weight\n",
      "\t features.7.1.bn3.weight\n",
      "\t features.7.1.bn3.bias\n",
      "\t features.7.2.conv1.weight\n",
      "\t features.7.2.bn1.weight\n",
      "\t features.7.2.bn1.bias\n",
      "\t features.7.2.conv2.weight\n",
      "\t features.7.2.bn2.weight\n",
      "\t features.7.2.bn2.bias\n",
      "\t features.7.2.conv3.weight\n",
      "\t features.7.2.bn3.weight\n",
      "\t features.7.2.bn3.bias\n",
      "\t downconv.0.weight\n",
      "\t downconv.0.bias\n",
      "\t classifier.0.weight\n",
      "\t classifier.0.bias\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using GPU \", device)   \n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are \n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.0001, momentum=0.9)\n",
    "scheduler_ft = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 1.4827 Acc: 0.4785 Dec: 0.4741\n",
      "val Loss: 1.1414 Acc: 0.6180 Dec: 0.5980\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 1.2745 Acc: 0.5589 Dec: 0.5528\n",
      "val Loss: 1.1172 Acc: 0.6379 Dec: 0.6291\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 1.2072 Acc: 0.5888 Dec: 0.5748\n",
      "val Loss: 1.0755 Acc: 0.6464 Dec: 0.6206\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 1.1753 Acc: 0.6002 Dec: 0.5819\n",
      "val Loss: 1.0482 Acc: 0.6564 Dec: 0.6288\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 1.1464 Acc: 0.6135 Dec: 0.5948\n",
      "val Loss: 1.1295 Acc: 0.6423 Dec: 0.6370\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 1.1092 Acc: 0.6251 Dec: 0.6056\n",
      "val Loss: 1.0693 Acc: 0.6540 Dec: 0.6558\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 1.0923 Acc: 0.6318 Dec: 0.6100\n",
      "val Loss: 1.1319 Acc: 0.6403 Dec: 0.6362\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 1.0748 Acc: 0.6394 Dec: 0.6157\n",
      "val Loss: 1.0374 Acc: 0.6602 Dec: 0.6479\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.9643 Acc: 0.6795 Dec: 0.6503\n",
      "val Loss: 1.0120 Acc: 0.6731 Dec: 0.6737\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.9196 Acc: 0.6976 Dec: 0.6680\n",
      "val Loss: 1.0170 Acc: 0.6758 Dec: 0.6752\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.8886 Acc: 0.7087 Dec: 0.6806\n",
      "val Loss: 1.0017 Acc: 0.6808 Dec: 0.6813\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.8799 Acc: 0.7096 Dec: 0.6830\n",
      "val Loss: 1.0041 Acc: 0.6828 Dec: 0.6840\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.8614 Acc: 0.7150 Dec: 0.6881\n",
      "val Loss: 1.0227 Acc: 0.6857 Dec: 0.6846\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.8469 Acc: 0.7231 Dec: 0.6906\n",
      "val Loss: 1.0169 Acc: 0.6919 Dec: 0.6881\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.8279 Acc: 0.7243 Dec: 0.6963\n",
      "val Loss: 1.0276 Acc: 0.6837 Dec: 0.6922\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.8203 Acc: 0.7300 Dec: 0.7029\n",
      "val Loss: 1.0169 Acc: 0.6866 Dec: 0.6860\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.8121 Acc: 0.7349 Dec: 0.7061\n",
      "val Loss: 1.0230 Acc: 0.6910 Dec: 0.6843\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.8065 Acc: 0.7341 Dec: 0.7036\n",
      "val Loss: 1.0170 Acc: 0.6887 Dec: 0.6898\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.8124 Acc: 0.7322 Dec: 0.7020\n",
      "val Loss: 1.0268 Acc: 0.6828 Dec: 0.6843\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.8038 Acc: 0.7357 Dec: 0.7080\n",
      "val Loss: 1.0230 Acc: 0.6854 Dec: 0.6881\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.8028 Acc: 0.7354 Dec: 0.7071\n",
      "val Loss: 1.0316 Acc: 0.6825 Dec: 0.6854\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.8070 Acc: 0.7362 Dec: 0.7058\n",
      "val Loss: 1.0348 Acc: 0.6875 Dec: 0.6893\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.7980 Acc: 0.7403 Dec: 0.7049\n",
      "val Loss: 1.0441 Acc: 0.6907 Dec: 0.6840\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.8028 Acc: 0.7372 Dec: 0.7020\n",
      "val Loss: 1.0190 Acc: 0.6843 Dec: 0.6904\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.8024 Acc: 0.7371 Dec: 0.7066\n",
      "val Loss: 1.0303 Acc: 0.6849 Dec: 0.6860\n",
      "\n",
      "Training complete in 424m 56s\n",
      "Best val Acc: 0.691901\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model_ft.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_ft.state_dict(),\n",
    "            }, model_savepath + 'wscnet.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
